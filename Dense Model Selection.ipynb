{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import recall_score, precision_score, precision_recall_fscore_support\n",
    "import sqlite3\n",
    "import keras\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/orig/cleaned_data_notEncoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_away'] = data['label_home'].apply(lambda x: -1 * x)\n",
    "\n",
    "data['home_won'] = data['label_home'].apply(lambda x: 1 if x == 1 else 0)\n",
    "data['home_loss'] = data['label_home'].apply(lambda x: 1 if x == -1 else 0)\n",
    "data['home_draw'] = data['label_home'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "data['away_won'] = data['label_away'].apply(lambda x: 1 if x == 1 else 0)\n",
    "data['away_loss'] = data['label_away'].apply(lambda x: 1 if x == -1 else 0)\n",
    "data['away_draw'] = data['label_away'].apply(lambda x: 1 if x == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match_data = pd.read_csv(\"data/orig/match_data.csv\")\n",
    "conn = sqlite3.connect('database.sqlite')\n",
    "res = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "match_data = pd.read_sql_query(\"SELECT * FROM Match\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_match_data = match_data[['season', 'match_api_id', 'date', 'home_team_api_id', 'away_team_api_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_season_date = pd.merge(data, relevant_match_data, on='match_api_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_home = data_with_season_date.groupby(['season', 'home_team_api_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "groups = []\n",
    "for name, group in grouped_home:\n",
    "    names += [name]\n",
    "    groups += [group]\n",
    "#     print(name)\n",
    "#     print(group.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groups = []\n",
    "for i in range(len(groups)):\n",
    "    df = groups[i]\n",
    "    name = names[i]\n",
    "    df['home_win_record'] = df['home_won'].cumsum() - (df['home_won'].cumsum() > 0)\n",
    "    df['home_loss_record'] = df['home_loss'].cumsum() - (df['home_loss'].cumsum() > 0)\n",
    "    df['home_draw_record'] = df['home_draw'].cumsum() - (df['home_draw'].cumsum() > 0)\n",
    "    df['season'] = name[0]\n",
    "    df['home_team_api_id'] = name[1]\n",
    "    new_groups += [df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home_stats = pd.concat(new_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home_stats[['label_home', 'home_win_record']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home_stats.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_away = df_home_stats.groupby(['season', 'away_team_api_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "groups = []\n",
    "for name, group in grouped_away:\n",
    "    names += [name]\n",
    "    groups += [group]\n",
    "#     print(name)\n",
    "#     print(group.shape)\n",
    "# print(names[4])\n",
    "# groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groups = []\n",
    "for i in range(len(groups)):\n",
    "    df = groups[i]\n",
    "    name = names[i]\n",
    "    df['away_win_record'] = df['away_won'].cumsum() - (df['away_won'].cumsum() > 0)\n",
    "    df['away_loss_record'] = df['away_loss'].cumsum() - (df['away_loss'].cumsum() > 0)\n",
    "    df['away_draw_record'] = df['away_draw'].cumsum() - (df['away_draw'].cumsum() > 0)\n",
    "    df['season'] = name[0]\n",
    "    df['away_team_api_id'] = name[1]\n",
    "    new_groups += [df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(new_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['season'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['season'] = pd.factorize(df_final['season'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns=['label_away', 'home_won', 'home_loss', 'home_draw',\n",
    "                                  'away_won', 'away_loss', 'away_draw', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns = ['season', 'home_win_record', 'home_loss_record', 'home_draw_record',\n",
    "                  'away_win_record', 'away_loss_record', 'away_draw_record']\n",
    "\n",
    "for i in range(1, 12):\n",
    "    for status in ['home_player', 'away_player']:\n",
    "        posX_name = status + '_X' + str(i)\n",
    "        posY_name = status + '_Y' + str(i)\n",
    "        \n",
    "        rating_name = status + '_' + str(i) + '_' + 'overall_rating'\n",
    "        desired_columns.extend([posX_name, posY_name, rating_name])\n",
    "\n",
    "df_rel = df_final[desired_columns]\n",
    "df_rel.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_final['label_home']\n",
    "df_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_rel.values\n",
    "y = df_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i + 1 for i in y]\n",
    "y = array(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) \n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    print(type(y_true))\n",
    "    print(tf.keras.backend.get_value(y_true))\n",
    "    recall_vals = recall_score(y_true, y_pred, average=None)\n",
    "    recall_vals = tf.convert_to_tensor(recall_vals, dtype=tf.float32)\n",
    "    return recall_vals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "\n",
    "def createToyModel(input_dim):\n",
    "    \n",
    "    xavier_init = glorot_normal(seed=54)\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(units=54, activation='elu', kernel_initializer=xavier_init))\n",
    "    model.add(Dense(units=27, activation='elu', kernel_initializer=xavier_init))\n",
    "    model.add(Dense(units=18, activation='elu', kernel_initializer=xavier_init))\n",
    "    model.add(Dense(units=9, activation='elu', kernel_initializer=xavier_init))\n",
    "    model.add(Dense(units=3, activation='softmax', kernel_initializer=xavier_init))\n",
    "    decay_rate = 3e-4 / 30\n",
    "    opt = Adam(lr=3e-4, decay=decay_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be careful about overwriting weights in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "seed(5)\n",
    "set_random_seed(10)\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "for i,f in enumerate(folds):\n",
    "    fpath = 'seung_best'+str(i)+'.h5' # change me\n",
    "    checkpoint = ModelCheckpoint(fpath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    train = f[0]\n",
    "    val = f[1]\n",
    "\n",
    "    X_fold_train, y_fold_train = X_train[train], y_train[train]\n",
    "    X_fold_val, y_fold_val = X_train[val], y_train[val]\n",
    "    \n",
    "    model = createToyModel(X_train.shape[1])\n",
    "    model.fit(x=X_fold_train, y=y_fold_train, batch_size=8, shuffle=True, epochs=30, \n",
    "    validation_data=(X_fold_val, y_fold_val), callbacks=callbacks_list)\n",
    "    \n",
    "    model.load_weights(fpath)\n",
    "    loss, acc = model.evaluate(X_fold_val, y_fold_val)\n",
    "    val_loss.append(loss)\n",
    "    val_acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Val losses:', val_loss)\n",
    "print('Val accs:', val_acc)\n",
    "print('Avg. val acc', np.mean(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "predicted = []\n",
    "model = createToyModel(X_train.shape[1])\n",
    "model.fit(x=X_fold_train, y=y_fold_train, batch_size=8, shuffle=True, epochs=1, \n",
    "    validation_data=(X_fold_val, y_fold_val), callbacks=callbacks_list)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(X_train, np.argmax(y_train, axis=1))\n",
    "for i,f in enumerate(folds):\n",
    "    fpath = 'fixed_best'+str(i)+'.h5' # CHANGE ME\n",
    "    train = f[0]\n",
    "    val = f[1]\n",
    "\n",
    "    X_fold_train, y_fold_train = X_train[train], y_train[train]\n",
    "    X_fold_val, y_fold_val = X_train[val], y_train[val]\n",
    "\n",
    "    \n",
    "    \n",
    "    model.load_weights(fpath)\n",
    "    loss, acc = model.evaluate(X_fold_train, y_fold_train)\n",
    "    y_pred = model.predict(x=X_test)\n",
    "    predicted.append(y_pred)\n",
    "    train_loss.append(loss)\n",
    "    train_acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.array(val_acc) - np.array(train_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=pd.Index(['Model ' + str(i) for i in range(5)] + ['Ensemble averaging']), columns=['precision', 'recall', 'f1', 'accuracy']).fillna(0)\n",
    "for i, p in enumerate(predicted):\n",
    "    p_score, r_score, f1_score, _ = precision_recall_fscore_support(y_test, one_hot(np.argmax(p, axis=1).astype(int), 3), average='weighted')\n",
    "    acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(p, axis=1))\n",
    "    df.loc['Model ' +str(i), 'precision'] = p_score\n",
    "    df.loc['Model ' +str(i), 'recall'] = r_score\n",
    "    df.loc['Model ' +str(i), 'f1'] = f1_score\n",
    "    df.loc['Model ' +str(i), 'accuracy'] = acc\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize predictions for all models\n",
    "predicted_arr = np.array(predicted)\n",
    "# for i in range(5):\n",
    "#     for j in range(predicted_arr.shape[1]):\n",
    "#         predicted_arr[i, j, :] /= np.linalg.norm(predicted_arr[i, j, :] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mean = np.mean(predicted_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.hist(np.argmax(predicted_mean, axis=1), color='r', alpha=0.5, label='pred')\n",
    "# plt.hist(np.argmax(y_test, axis=1), color='yellow', alpha=0.3, label='true')\n",
    "p_score, r_score, f1_score, _ = precision_recall_fscore_support(y_test, one_hot(np.argmax(predicted_mean, axis=1).astype(int), 3), average='weighted')\n",
    "acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(predicted_mean, axis=1))\n",
    "df.loc['Ensemble averaging', 'precision'] = p_score\n",
    "df.loc['Ensemble averaging', 'recall'] = r_score\n",
    "df.loc['Ensemble averaging', 'f1'] = f1_score\n",
    "df.loc['Ensemble averaging', 'accuracy'] = acc\n",
    "\n",
    "# df.to_csv('deeper_prediction_scores.csv') # BE CAREFUL ABOUT OVERWRITING\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds = pd.read_csv('odds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
