{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell loads the encoded images and splits them into two (home/away). The images are of shape 9x11x3 and are inputs for the siamese net. The second cell loads the match data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "formations = np.load(\"data/formations.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(formation_input_shape, reduced_attr_input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    home_formation_input = Input(formation_input_shape)\n",
    "    away_formation_input = Input(formation_input_shape)\n",
    "    \n",
    "    attr_input = Input(reduced_attr_input_shape)\n",
    "    \n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(16, (2,2), activation='relu', input_shape=formation_input_shape))\n",
    "    cnn.add(MaxPooling2D())\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(64, activation='sigmoid'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    cnn_encoded_home = cnn(home_formation_input)\n",
    "    cnn_encoded_away = cnn(away_formation_input)\n",
    "    \n",
    "    # attribute encoding MLP\n",
    "    mlp0 = Sequential()\n",
    "    mlp0.add(Dense(32, activation='sigmoid', input_shape=reduced_attr_input_shape))\n",
    "    mlp0.add(Dense(32, activation='relu'))\n",
    "    mlp0.add(Dense(64, activation='tanh'))\n",
    "    \n",
    "    mlp_encoded_attr = mlp0(attr_input)\n",
    "    \n",
    "    # prediction generating MLP\n",
    "    concat = Concatenate()\n",
    "    encoded_input = concat([mlp_encoded_attr, cnn_encoded_home, cnn_encoded_away])\n",
    "    mlp1 = Sequential()\n",
    "    mlp1.add(Dense(64, activation='tanh',input_shape=(64*3,)))\n",
    "    mlp1.add(Dense(32, activation='relu'))\n",
    "    mlp1.add(Dense(3,activation='softmax'))\n",
    "    \n",
    "    # Add a dense layer with a softmax unit to generate the probabilities of home and away team winning\n",
    "    prediction = mlp1(encoded_input)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[attr_input, home_formation_input, away_formation_input], outputs=prediction)\n",
    "    optimizer = Adam(lr = 0.00005)\n",
    "    siamese_net.compile(loss=\"categorical_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Siamese Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************5*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 4s 216us/step - loss: 1.0755 - acc: 0.4139 - val_loss: 1.3673 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 4s 197us/step - loss: 1.0744 - acc: 0.4157 - val_loss: 1.3927 - val_acc: 0.0373\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 4s 205us/step - loss: 1.0740 - acc: 0.4162 - val_loss: 1.3529 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 4s 202us/step - loss: 1.0734 - acc: 0.4154 - val_loss: 1.3652 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 4s 197us/step - loss: 1.0733 - acc: 0.4147 - val_loss: 1.3403 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 4s 215us/step - loss: 1.0729 - acc: 0.4160 - val_loss: 1.2989 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 4s 227us/step - loss: 1.0733 - acc: 0.4155 - val_loss: 1.3789 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 4s 214us/step - loss: 1.0729 - acc: 0.4166 - val_loss: 1.3142 - val_acc: 8.5855e-04\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 4s 210us/step - loss: 1.0726 - acc: 0.4168 - val_loss: 1.3075 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 4s 202us/step - loss: 1.0723 - acc: 0.4165 - val_loss: 1.3440 - val_acc: 0.0000e+00\n",
      "******************10*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 4s 230us/step - loss: 1.0766 - acc: 0.4094 - val_loss: 1.3503 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 4s 209us/step - loss: 1.0749 - acc: 0.4127 - val_loss: 1.3566 - val_acc: 4.2928e-04\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 4s 216us/step - loss: 1.0741 - acc: 0.4153 - val_loss: 1.2651 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 4s 218us/step - loss: 1.0739 - acc: 0.4137 - val_loss: 1.3888 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 4s 214us/step - loss: 1.0737 - acc: 0.4128 - val_loss: 1.3841 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 4s 210us/step - loss: 1.0734 - acc: 0.4127 - val_loss: 1.3261 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 4s 216us/step - loss: 1.0728 - acc: 0.4166 - val_loss: 1.3317 - val_acc: 0.0094\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 4s 211us/step - loss: 1.0724 - acc: 0.4141 - val_loss: 1.3498 - val_acc: 0.0178\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 4s 206us/step - loss: 1.0718 - acc: 0.4146 - val_loss: 1.3649 - val_acc: 0.0030\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 4s 207us/step - loss: 1.0724 - acc: 0.4168 - val_loss: 1.3709 - val_acc: 4.2928e-04\n",
      "******************25*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 5s 249us/step - loss: 1.0753 - acc: 0.4139 - val_loss: 1.3304 - val_acc: 0.0049\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 4s 221us/step - loss: 1.0719 - acc: 0.4208 - val_loss: 1.4299 - val_acc: 0.1232\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 4s 230us/step - loss: 1.0691 - acc: 0.4317 - val_loss: 1.4347 - val_acc: 0.1715\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 4s 224us/step - loss: 1.0639 - acc: 0.4440 - val_loss: 1.4121 - val_acc: 0.1736\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 4s 239us/step - loss: 1.0594 - acc: 0.4532 - val_loss: 1.3210 - val_acc: 0.1912\n",
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 4s 229us/step - loss: 1.0551 - acc: 0.4531 - val_loss: 1.4249 - val_acc: 0.2146\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 5s 245us/step - loss: 1.0517 - acc: 0.4577 - val_loss: 1.3433 - val_acc: 0.1992\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 5s 244us/step - loss: 1.0491 - acc: 0.4581 - val_loss: 1.4597 - val_acc: 0.2187\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 4s 227us/step - loss: 1.0470 - acc: 0.4620 - val_loss: 1.4704 - val_acc: 0.1721\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 4s 230us/step - loss: 1.0456 - acc: 0.4612 - val_loss: 1.5537 - val_acc: 0.2091\n",
      "******************50*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 5s 261us/step - loss: 1.0761 - acc: 0.4139 - val_loss: 1.2492 - val_acc: 0.0476\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 4s 237us/step - loss: 1.0728 - acc: 0.4172 - val_loss: 1.2973 - val_acc: 0.0120\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 5s 264us/step - loss: 1.0714 - acc: 0.4217 - val_loss: 1.3259 - val_acc: 0.0713\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 5s 243us/step - loss: 1.0670 - acc: 0.4344 - val_loss: 1.3775 - val_acc: 0.1805\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 5s 249us/step - loss: 1.0612 - acc: 0.4517 - val_loss: 1.4387 - val_acc: 0.1934\n",
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 4s 226us/step - loss: 1.0561 - acc: 0.4568 - val_loss: 1.5357 - val_acc: 0.2228\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 4s 228us/step - loss: 1.0519 - acc: 0.4576 - val_loss: 1.4066 - val_acc: 0.1973\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 4s 236us/step - loss: 1.0482 - acc: 0.4622 - val_loss: 1.4657 - val_acc: 0.2252\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 4s 235us/step - loss: 1.0452 - acc: 0.4621 - val_loss: 1.6431 - val_acc: 0.2550\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 4s 230us/step - loss: 1.0436 - acc: 0.4641 - val_loss: 1.5696 - val_acc: 0.2385\n",
      "******************100*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 5s 266us/step - loss: 1.0749 - acc: 0.4109 - val_loss: 1.3222 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 4s 237us/step - loss: 1.0702 - acc: 0.4240 - val_loss: 1.2934 - val_acc: 0.1099\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 5s 249us/step - loss: 1.0652 - acc: 0.4438 - val_loss: 1.3206 - val_acc: 0.1436\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 4s 239us/step - loss: 1.0589 - acc: 0.4567 - val_loss: 1.3375 - val_acc: 0.1919\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 5s 243us/step - loss: 1.0550 - acc: 0.4598 - val_loss: 1.2743 - val_acc: 0.2393\n",
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 5s 251us/step - loss: 1.0511 - acc: 0.4611 - val_loss: 1.3284 - val_acc: 0.2331\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 5s 247us/step - loss: 1.0484 - acc: 0.4641 - val_loss: 1.3481 - val_acc: 0.2631\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 4s 241us/step - loss: 1.0459 - acc: 0.4649 - val_loss: 1.3801 - val_acc: 0.2586\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 5s 242us/step - loss: 1.0433 - acc: 0.4679 - val_loss: 1.3648 - val_acc: 0.2520\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 5s 247us/step - loss: 1.0409 - acc: 0.4706 - val_loss: 1.4087 - val_acc: 0.2481\n",
      "******************250*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 6s 298us/step - loss: 1.0746 - acc: 0.4154 - val_loss: 1.3273 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 5s 250us/step - loss: 1.0699 - acc: 0.4230 - val_loss: 1.2907 - val_acc: 0.2211\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 5s 268us/step - loss: 1.0631 - acc: 0.4465 - val_loss: 1.3927 - val_acc: 0.1788\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 5s 287us/step - loss: 1.0550 - acc: 0.4625 - val_loss: 1.4657 - val_acc: 0.2610\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 5s 254us/step - loss: 1.0490 - acc: 0.4646 - val_loss: 1.4284 - val_acc: 0.2254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 4s 240us/step - loss: 1.0453 - acc: 0.4684 - val_loss: 1.4503 - val_acc: 0.2511\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 5s 273us/step - loss: 1.0412 - acc: 0.4753 - val_loss: 1.3924 - val_acc: 0.2483\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 4s 236us/step - loss: 1.0384 - acc: 0.4745 - val_loss: 1.4687 - val_acc: 0.2666\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 5s 260us/step - loss: 1.0357 - acc: 0.4752 - val_loss: 1.4618 - val_acc: 0.2501\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 5s 250us/step - loss: 1.0326 - acc: 0.4790 - val_loss: 1.5536 - val_acc: 0.2460\n",
      "******************500*********************\n",
      "Train on 18633 samples, validate on 4659 samples\n",
      "Epoch 1/10\n",
      "18633/18633 [==============================] - 6s 300us/step - loss: 1.0744 - acc: 0.4173 - val_loss: 1.4542 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "18633/18633 [==============================] - 5s 258us/step - loss: 1.0694 - acc: 0.4267 - val_loss: 1.3124 - val_acc: 0.0783\n",
      "Epoch 3/10\n",
      "18633/18633 [==============================] - 5s 256us/step - loss: 1.0601 - acc: 0.4547 - val_loss: 1.2877 - val_acc: 0.1354\n",
      "Epoch 4/10\n",
      "18633/18633 [==============================] - 5s 263us/step - loss: 1.0508 - acc: 0.4642 - val_loss: 1.3608 - val_acc: 0.2631\n",
      "Epoch 5/10\n",
      "18633/18633 [==============================] - 5s 260us/step - loss: 1.0434 - acc: 0.4700 - val_loss: 1.4893 - val_acc: 0.2509\n",
      "Epoch 6/10\n",
      "18633/18633 [==============================] - 5s 271us/step - loss: 1.0376 - acc: 0.4772 - val_loss: 1.5358 - val_acc: 0.2739\n",
      "Epoch 7/10\n",
      "18633/18633 [==============================] - 5s 272us/step - loss: 1.0330 - acc: 0.4775 - val_loss: 1.5070 - val_acc: 0.2591\n",
      "Epoch 8/10\n",
      "18633/18633 [==============================] - 5s 262us/step - loss: 1.0277 - acc: 0.4857 - val_loss: 1.4800 - val_acc: 0.2477\n",
      "Epoch 9/10\n",
      "18633/18633 [==============================] - 5s 271us/step - loss: 1.0242 - acc: 0.4873 - val_loss: 1.5245 - val_acc: 0.2537\n",
      "Epoch 10/10\n",
      "18633/18633 [==============================] - 5s 262us/step - loss: 1.0200 - acc: 0.4874 - val_loss: 1.5875 - val_acc: 0.2419\n"
     ]
    }
   ],
   "source": [
    "# split images into home/away teams\n",
    "formations_home = formations[:, 0, :, :, :]\n",
    "formation_away = formations[:, 1, :, :, :]\n",
    "n = formations.shape[0]\n",
    "m = int(n * .8)\n",
    "\n",
    "formations_home_train = formations[:m, 0, :, :, :]\n",
    "formations_away_train = formations[:m, 1, :, :, :]\n",
    "formations_home_test = formations[m:, 0, :, :, :]\n",
    "formations_away_test = formations[m:, 1, :, :, :]\n",
    "y = pd.read_csv(\"data/y_resampled_formation.csv\").values\n",
    "\n",
    "for num_features in [5, 10, 25, 50, 100, 250, 500]:\n",
    "    print(\"******************\" + str(num_features) + \"*********************\")\n",
    "    file_name_data_formation = \"data/pca/\" + str(num_features) + \"/data_formation_\" + str(num_features) + \".npy\"\n",
    "    file_name_test_data_formation = \"data/pca/\" + str(num_features) + \"/test_data_formation_\" + str(num_features) + \".npy\"\n",
    "    \n",
    "    X = np.load(file_name_data_formation)\n",
    "    reduced_attr_train = X[:m]\n",
    "    reduced_attr_test = X[m:]\n",
    "    label_train = y[:m]\n",
    "    label_test = y[m:]\n",
    "    siamese_net = get_siamese_model(formations_home.shape[1:], (X.shape[1],))\n",
    "    siamese_net.fit(x=[reduced_attr_train, formations_home_train, formations_away_train],\n",
    "                    y=label_train,\n",
    "                    validation_data=([reduced_attr_test, formations_home_test, formations_away_test], label_test),\n",
    "                    batch_size=8, epochs=10)\n",
    "    # siamese_net.predict(x=[reduced_attr_train, formations_home_train, formations_away_train])\n",
    "    \n",
    "# attr_home_train = X[:m, :429]\n",
    "# attr_away_train = X[:m, 429:]\n",
    "# attr_home_test = X[m:, :429]\n",
    "# attr_away_test= X[m:, 429:]\n",
    "\n",
    "\n",
    "# # print(attr_home_train.shape)\n",
    "# # print(attr_away_train.shape)\n",
    "# label_train = y[:m]\n",
    "# label_test = y[m:]\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
